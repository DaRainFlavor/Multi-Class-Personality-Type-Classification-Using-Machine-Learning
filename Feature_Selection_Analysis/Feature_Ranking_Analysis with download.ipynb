{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Feature Selection Analysis: 16 Personality Types\n",
                "\n",
                "This notebook performs feature selection to identify the most significant questions for predicting MBTI personality types. It uses **XGBoost** to rank features by importance and then performs a recursive analysis to determine the optimal number of questions to keep while maintaining high accuracy.\n",
                "\n",
                "### **Objective**\n",
                "Reduce the questionnaire length (from 60 questions) without significantly sacrificing prediction accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries (if running in a fresh environment)\n",
                "!pip install xgboost pandas seaborn scikit-learn numpy matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import accuracy_score\n",
                "from xgboost import XGBClassifier\n",
                "import os\n",
                "\n",
                "# Set Plot Style\n",
                "sns.set(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "# NOTE: Upload '16P.csv' or '16P_cleaned.csv' to the Colab runtime before running.\n",
                "file_path = '16P_cleaned.csv' \n",
                "\n",
                "# Fallback if user uses the original filename\n",
                "if not os.path.exists(file_path):\n",
                "    if os.path.exists('16P.csv'):\n",
                "        file_path = '16P.csv'\n",
                "    else:\n",
                "        print(\"\\u26A0\\uFE0F WARNING: Dataset file not found. Please upload '16P_cleaned.csv' or '16P.csv'.\")\n",
                "\n",
                "if os.path.exists(file_path):\n",
                "    try:\n",
                "        # Try default UTF-8 first\n",
                "        df = pd.read_csv(file_path)\n",
                "    except UnicodeDecodeError:\n",
                "        print(\"UTF-8 decode failed, trying cp1252 (common for Windows files)...\")\n",
                "        # Fallback to cp1252 which handles smart quotes/special chars often found in survey data\n",
                "        df = pd.read_csv(file_path, encoding='cp1252')\n",
                "        \n",
                "    print(f\"Success: Loaded {file_path} with shape {df.shape}\")\n",
                "else:\n",
                "    df = pd.DataFrame() # Empty placeholder"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing\n",
                "We remove the identifiers and any derived scores, keeping only the raw question responses and the target 'Personality'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not df.empty:\n",
                "    # Drop identifier if exists\n",
                "    if 'Response Id' in df.columns:\n",
                "        df = df.drop(columns=['Response Id'])\n",
                "\n",
                "    # Identify columns to drop (derived scores, etc.)\n",
                "    cols_to_drop = ['Personality', 'E_I_score', 'S_N_score', 'T_F_score', 'J_P_score', \n",
                "                    'E_I_strength', 'S_N_strength', 'T_F_strength', 'J_P_strength',\n",
                "                    'is_Extraverted', 'is_Intuitive', 'is_Feeling', 'is_Judging', \n",
                "                    'Consistency', 'Original_Personality']\n",
                "    \n",
                "    # Filter columns that actually exist in the dataframe\n",
                "    cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
                "    \n",
                "    X = df.drop(columns=cols_to_drop)\n",
                "    y = df['Personality']\n",
                "    \n",
                "    feature_names = X.columns.tolist()\n",
                "    print(f\"Features (Questions): {len(feature_names)}\")\n",
                "    print(f\"Target Classes: {y.nunique()} ({', '.join(y.unique()[:5])}...)\")\n",
                "\n",
                "    # Encode Target\n",
                "    le = LabelEncoder()\n",
                "    y_encoded = le.fit_transform(y)\n",
                "    \n",
                "    # Split data (70% Train, 15% Validation, 15% Test)\n",
                "    TEST_SIZE = 0.15      # 15% for test\n",
                "    VAL_SIZE = 0.176      # 15% of remaining 85% = 15% of total\n",
                "\n",
                "    # First split: 85% train+val, 15% test\n",
                "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
                "        X, y_encoded, \n",
                "        test_size=TEST_SIZE, \n",
                "        random_state=42, \n",
                "        stratify=y_encoded\n",
                "    )\n",
                "\n",
                "    # Second split: 70% train, 15% val (of total)\n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_temp, y_temp, \n",
                "        test_size=VAL_SIZE, \n",
                "        random_state=42, \n",
                "        stratify=y_temp\n",
                "    )\n",
                "\n",
                "    print(f\"Training set:   {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
                "    print(f\"Validation set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
                "    print(f\"Test set:       {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Importance with XGBoost\n",
                "We train a robust XGBoost classifier on all 60 features to establish a baseline and extract feature importance scores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not df.empty:\n",
                "    print(\"Training XGBoost model (Baseline)...\")\n",
                "    \n",
                "    # Configure XGBoost (matching ML_Comparison_Analysis.ipynb)\n",
                "    xgb_params = {\n",
                "        'n_estimators': 500,\n",
                "        'learning_rate': 0.1,\n",
                "        'max_depth': 6,\n",
                "        'min_child_weight': 1,\n",
                "        'subsample': 0.8,\n",
                "        'colsample_bytree': 0.8,\n",
                "        'objective': 'multi:softprob',\n",
                "        'num_class': len(le.classes_),\n",
                "        'random_state': 42,\n",
                "        'n_jobs': -1,\n",
                "        'verbosity': 0\n",
                "    }\n",
                "    \n",
                "    print(\"Hyperparameters:\")\n",
                "    for key, value in xgb_params.items():\n",
                "        if key not in ['verbosity', 'n_jobs']:\n",
                "            print(f\"  {key}: {value}\")\n",
                "\n",
                "    # Train model with early stopping enabled (using validation set)\n",
                "    model = XGBClassifier(**xgb_params, early_stopping_rounds=15)\n",
                "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
                "    \n",
                "    baseline_acc = accuracy_score(y_test, model.predict(X_test))\n",
                "    print(f\"Baseline Accuracy (All 60 Features): {baseline_acc:.4f} (Should match ML Comparison Notebook)\")\n",
                "\n",
                "    # Extract Feature Importances\n",
                "    importances = model.feature_importances_\n",
                "    feature_importance_df = pd.DataFrame({\n",
                "        'Feature': feature_names,\n",
                "        'Importance': importances\n",
                "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
                "    \n",
                "    print(\"Top 10 Most Important Features:\")\n",
                "    print(feature_importance_df.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Top 20 Features\n",
                "if not df.empty:\n",
                "    plt.figure(figsize=(12, 10))\n",
                "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20), palette='viridis')\n",
                "    plt.title('Top 20 Most Significant Questions for Personality Prediction')\n",
                "    plt.xlabel('Importance Score')\n",
                "    plt.ylabel('Question')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Recursive Stepwise Analysis (Top N)\n",
                "We will now iteratively train models using only the Top N features. we check N=1 to 20 individually to observe the critical Pareto distribution, and then in larger steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "# Check Top 1-20 individually, then every 5\n",
                "n_features_list = list(range(1, 21)) + list(range(25, 65, 5))\n",
                "\n",
                "if not df.empty:\n",
                "    print(\"Starting Stepwise Analysis...\")\n",
                "    for n in n_features_list:\n",
                "        top_n_feats = feature_importance_df['Feature'].head(n).tolist()\n",
                "        \n",
                "        # Subset data to just Top N columns\n",
                "        X_train_sub = X_train[top_n_feats]\n",
                "        X_test_sub = X_test[top_n_feats]\n",
                "        X_val_sub = X_val[top_n_feats]\n",
                "        \n",
                "        # Train sub-model with same hyperparameters and early stopping\n",
                "        model_sub = XGBClassifier(**xgb_params, early_stopping_rounds=15)\n",
                "        model_sub.fit(X_train_sub, y_train, eval_set=[(X_val_sub, y_val)], verbose=False)\n",
                "        \n",
                "        acc = accuracy_score(y_test, model_sub.predict(X_test_sub))\n",
                "        \n",
                "        results.append({'N_Features': n, 'Accuracy': acc})\n",
                "        print(f\"-> Top {n} Questions: Accuracy = {acc:.4f}\")\n",
                "\n",
                "    results_df = pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not df.empty:\n",
                "    # Plot Accuracy vs N Features\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.lineplot(x='N_Features', y='Accuracy', data=results_df, marker='o', linewidth=2.5, color='darkblue')\n",
                "    plt.axhline(y=baseline_acc, color='r', linestyle='--', label=f'Baseline (60): {baseline_acc:.4f}')\n",
                "    plt.title('Accuracy vs Number of Top Questions Kept')\n",
                "    plt.xlabel('Number of Questions')\n",
                "    plt.ylabel('Test Accuracy')\n",
                "    plt.xticks(list(range(0, 65, 5)))\n",
                "    plt.grid(True)\n",
                "    plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Recommendation\n",
                "Based on the analysis above, we suggest a cut-off point."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not df.empty:\n",
                "    # Define threshold as 98% of baseline accuracy\n",
                "    threshold_98 = 0.98 * baseline_acc\n",
                "    threshold_95 = 0.95 * baseline_acc\n",
                "    \n",
                "    rec_98 = results_df[results_df['Accuracy'] >= threshold_98]['N_Features'].min()\n",
                "    rec_95 = results_df[results_df['Accuracy'] >= threshold_95]['N_Features'].min()\n",
                "    \n",
                "    print(\"=\"*50)\n",
                "    print(\"RECOMMENDATION\")\n",
                "    print(\"=\"*50)\n",
                "    print(f\"Baseline Accuracy (60 Questions): {baseline_acc:.4f}\")\n",
                "    print(f\"- To maintain >98% accuracy ({threshold_98:.4f}): Keep Top {rec_98} Questions\")\n",
                "    print(f\"- To maintain >95% accuracy ({threshold_95:.4f}): Keep Top {rec_95} Questions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export Models for Website Deployment\n",
                "This section exports both the **full model (60 questions)** and **short model (35 questions)** to ONNX format for website deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install ONNX libraries\n",
                "!pip install onnx onnxruntime onnxmltools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === COMPLETE EXPORT SOLUTION (with all fixes) ===\n",
                "import onnxmltools\n",
                "from onnxmltools.convert.common.data_types import FloatTensorType\n",
                "import onnxruntime as ort\n",
                "import json\n",
                "\n",
                "# Step 1: Rename columns to f0, f1, f2... (REQUIRED for ONNX conversion)\n",
                "print(\"Step 1: Renaming columns to numeric format for ONNX compatibility...\")\n",
                "feature_mapping = {f'f{i}': feat for i, feat in enumerate(feature_names)}\n",
                "X_train_r = X_train.copy()\n",
                "X_val_r = X_val.copy()  \n",
                "X_test_r = X_test.copy()\n",
                "X_train_r.columns = [f'f{i}' for i in range(60)]\n",
                "X_val_r.columns = [f'f{i}' for i in range(60)]\n",
                "X_test_r.columns = [f'f{i}' for i in range(60)]\n",
                "print(\"Done!\")\n",
                "\n",
                "# Step 2: Retrain FULL model with numeric column names\n",
                "print(\"\\nStep 2: Training Full Model (60 questions) with numeric feature names...\")\n",
                "model_full = XGBClassifier(**xgb_params, early_stopping_rounds=15)\n",
                "model_full.fit(X_train_r, y_train, eval_set=[(X_val_r, y_val)], verbose=False)\n",
                "full_acc = accuracy_score(y_test, model_full.predict(X_test_r))\n",
                "print(f\"Full Model Accuracy: {full_acc:.4f} ({full_acc*100:.2f}%)\")\n",
                "\n",
                "# Step 3: Retrain SHORT model (35 questions) with SEQUENTIAL feature names (f0-f34)\n",
                "# CRITICAL: ONNX requires sequential feature names, not the original indices!\n",
                "TOP_N = 35\n",
                "top_35_orig = feature_importance_df['Feature'].head(TOP_N).tolist()\n",
                "top_35_idx = [feature_names.index(f) for f in top_35_orig]  # Original indices for reference\n",
                "top_35_cols_orig = [f'f{i}' for i in top_35_idx]  # Columns in the renamed full dataset\n",
                "\n",
                "# Create SHORT dataset with SEQUENTIAL names (f0, f1, f2, ..., f34)\n",
                "X_train_short = X_train_r[top_35_cols_orig].copy()\n",
                "X_val_short = X_val_r[top_35_cols_orig].copy()\n",
                "X_test_short = X_test_r[top_35_cols_orig].copy()\n",
                "X_train_short.columns = [f'f{i}' for i in range(TOP_N)]  # Rename to f0, f1, f2, ...\n",
                "X_val_short.columns = [f'f{i}' for i in range(TOP_N)]\n",
                "X_test_short.columns = [f'f{i}' for i in range(TOP_N)]\n",
                "\n",
                "print(f\"\\nStep 3: Training Short Model ({TOP_N} questions)...\")\n",
                "print(f\"Using features with SEQUENTIAL names: f0 through f{TOP_N-1}\")\n",
                "model_short = XGBClassifier(**xgb_params, early_stopping_rounds=15)\n",
                "model_short.fit(X_train_short, y_train, eval_set=[(X_val_short, y_val)], verbose=False)\n",
                "short_acc = accuracy_score(y_test, model_short.predict(X_test_short))\n",
                "print(f\"Short Model Accuracy: {short_acc:.4f} ({short_acc*100:.2f}%)\")\n",
                "print(f\"Accuracy Retention: {(short_acc/full_acc)*100:.1f}% of full model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: Convert models to ONNX and save all files\n",
                "print(\"=\"*50)\n",
                "print(\"Step 4: EXPORTING FILES FOR WEBSITE\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "def to_onnx(m, n, path):\n",
                "    onx = onnxmltools.convert_xgboost(m, initial_types=[('float_input', FloatTensorType([None, n]))], target_opset=12)\n",
                "    with open(path, 'wb') as f: \n",
                "        f.write(onx.SerializeToString())\n",
                "    ort.InferenceSession(path)  # Verify it loads\n",
                "    print(f'\\u2705 {path}')\n",
                "\n",
                "# Export ONNX models\n",
                "to_onnx(model_full, 60, 'mbti_model.onnx')\n",
                "to_onnx(model_short, TOP_N, 'mbti_model_short.onnx')\n",
                "\n",
                "# Export JSON metadata\n",
                "with open('labels.json', 'w') as f: \n",
                "    json.dump(list(le.classes_), f)\n",
                "print('\\u2705 labels.json')\n",
                "\n",
                "with open('top_35_questions.json', 'w') as f: \n",
                "    json.dump({\n",
                "        'count': TOP_N, \n",
                "        'indices': top_35_idx, \n",
                "        'features': top_35_orig\n",
                "    }, f, indent=2)\n",
                "print('\\u2705 top_35_questions.json')\n",
                "\n",
                "with open('all_questions.json', 'w') as f: \n",
                "    json.dump(feature_names, f, indent=2)\n",
                "print('\\u2705 all_questions.json')\n",
                "\n",
                "with open('accuracy_report.json', 'w') as f: \n",
                "    json.dump({\n",
                "        'full_model_accuracy': round(full_acc, 4), \n",
                "        'short_model_accuracy': round(short_acc, 4),\n",
                "        'accuracy_retention_percent': round((short_acc/full_acc)*100, 2)\n",
                "    }, f, indent=2)\n",
                "print('\\u2705 accuracy_report.json')\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Full Model (60Q):  {full_acc*100:.2f}%\")\n",
                "print(f\"Short Model (35Q): {short_acc*100:.2f}%\")\n",
                "print(f\"Accuracy Retention: {(short_acc/full_acc)*100:.1f}%\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Auto-download all files (Colab)\n",
                "from google.colab import files\n",
                "\n",
                "print('Downloading files...')\n",
                "print('(Click \"Allow\" if prompted by browser)')\n",
                "\n",
                "files.download('mbti_model.onnx')          # Full model (60 questions)\n",
                "files.download('mbti_model_short.onnx')    # Short model (35 questions)\n",
                "files.download('labels.json')              # Class labels\n",
                "files.download('top_35_questions.json')    # Short model question indices\n",
                "files.download('all_questions.json')       # All 60 questions\n",
                "files.download('accuracy_report.json')     # Accuracy comparison\n",
                "\n",
                "print('\\n\\u2705 All files downloaded!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udcc1 Where to Place Downloaded Files\n",
                "\n",
                "After downloading, place the files in your repo:\n",
                "\n",
                "| File | Location |\n",
                "|------|----------|\n",
                "| `mbti_model.onnx` | `mbti-quiz/api/` |\n",
                "| `mbti_model_short.onnx` | `mbti-quiz/api/` |\n",
                "| `labels.json` | `mbti-quiz/api/` |\n",
                "| `top_35_questions.json` | `mbti-quiz/api/` |\n",
                "| `all_questions.json` | `mbti-quiz/api/` (optional) |\n",
                "| `accuracy_report.json` | `Feature_Selection_Analysis/` (for reference) |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}